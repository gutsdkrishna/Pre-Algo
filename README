# Ahoum **rate‑v2** Micro‑Service

> **Purpose**  Provide an entirely self‑hosted (`$0 external cost`) HTTP endpoint (`POST /rate_v2`) that converts a 9‑10‑turn seeker dialogue into probabilistic scores for \~200 psychological‑spiritual facets, ready to scale up to 40 000.

---

## 1  High‑Level Architecture

```text
 ┌──────────────┐     1️⃣ HTTP JSON       ┌───────────────────┐
 │  Client / UI │ ───────────────────▶ │  Flask / Gunicorn │
 └──────────────┘                      └────────┬──────────┘
                                               │ 2️⃣ Pre‑proc
                                               ▼
                                       preprocess.py (clean)
                                               │ 3️⃣ Parallel
                                               │   feature passes
                                               ▼
                           ┌──────────────────────────────────────────┐
                           │  emotion.py   → 28‑dim GoEmotions logits│
Dialogue list  ─────────▶ │  liwc_features → 90 LIWC counts          │
                           └──────────────────────────────────────────┘
                                               │
                                               ▼ 4️⃣ Encode context
                                       encoder.py (HRAN 256‑d)
                                               │
                                               ▼ 5️⃣ Multi‑task heads
                                       predictor.py
                                               │
                               (optional)      ▼ 6️⃣ KG bias
                               kg.py  ◀────────┘
                                               │
                               (optional)      ▼ 7️⃣ Re‑rank 30 best
                               reranker.py ◀──┘   (Llama 3 / Mistral)
                                               │
                                               ▼ 8️⃣ Platt calibrate
                                       calibration.py
                                               │
                                               ▼ 9️⃣ JSON build
                                       Flask serializer
```

Latency on 8‑core CPU ≈ **0.9 s**; < 0.5 s with mid‑range GPU.

---

## 2  Directory Layout

| Path                                | Purpose                                                            |
| ----------------------------------- | ------------------------------------------------------------------ |
| `app/`                              | All Python source files (import‑safe).                             |
| `models/`                           | Place **quantised** GGUF Llama 3 / Mistral + `facet_predictor.pt`. |
| `Dockerfile` / `docker‑compose.yml` | Containerised deployment.                                          |
| `requirements.txt`                  | Fully pinned dependency list.                                      |

---

## 3  Key Components

### 3.1  Pre‑processing (`preprocess.py`)

* Normalises HTML entities, accents, emojis, repeated whitespace.
* Returns lower‑case clean text for downstream modules.

### 3.2  Utterance‑level Emotion (`emotion.py`)

* Local **RoBERTa‑GoEmotions** (MIT licence) → 28 logits per utterance.
* Exported to ONNX for CPU inference (< 125 MB).

### 3.3  Psycholinguistic Counts (`liwc_features.py`)

* Uses open `pyliwc` lexicon → \~90 category frequencies.

### 3.4  Hierarchical Encoder (`encoder.py`)

* HRAN: GRU+attention **word → utterance → dialogue** → 256‑d context vector `h`.

### 3.5  Facet Predictor (`predictor.py`)

* Multi‑task: Big‑Five regression, 200‑facet sigmoid, emotion reconstruction.
* Trained in PyTorch Lightning (weights shipped as `facet_predictor.pt`).



### 3.6  LLM Re‑rank (`reranker.py`) *optional but recommended*

* Local **Llama 3‑8B** or **Mistral‑7B‑Instruct** (Q4\_0) scores top‑30 candidate facets 0‑100.

### 3.7  Calibration (`calibration.py`)

* Platt logistic scaling turns raw+LLM fusion into calibrated probabilities.

---

## 4  API Spec

* **POST `/rate_v2`**
  `Content‑Type: application/json`

### Request Body

```json
{
  "conversation": [
    {"speaker": "AI",   "text": "Hi there!"},
    {"speaker": "User", "text": "I feel anxious about exams."},
    …(up to ≈10 turns)…
  ],
  "user_id": "123"   // optional, for KG personalisation
}
```

### Success Response

```json
{
  "facet_scores": [
    {"facet_id": 17, "score": 0.88},
    {"facet_id": 42, "score": 0.23},
    … up to 30 facets …
  ],
  "processing_ms": 842
}
```

*Scores are **0‑1** calibrated; map to 1‑10 buckets on the client if needed.*

---

## 5  Running the Service

### 5.1  Docker (recommended)

```bash
# build images
$ docker compose build
# launch Postgres + API
$ docker compose up -d
# once DB is live, enable pgvector
$ docker exec -it <db‑container> psql -U pg -d ahoum -c "CREATE EXTENSION pgvector;"
```

The API listens on **`http://localhost:8000`**.

### 5.2  Local venv

```bash
$ python -m venv venv && source venv/bin/activate
$ pip install -r requirements.txt
$ export PYTHONPATH=$PWD/app
$ python -m app.main  # dev server
```

---

## 6  Environment Variables

| Variable   | Description                                | Default                                |
| ---------- | ------------------------------------------ | -------------------------------------- |
| `LLM_PATH` | Path to GGUF model for reranker            | `models/mistral-7b-instruct.Q4_0.gguf` |
| `DB_URL`   | SQLAlchemy/psycopg URI for Postgres        | `postgresql:///ahoum`                  |
| `N_FACETS` | Number of facets in current ontology slice | `200`                                  |

---

## 7  Training & Fine‑tuning (brief)

1. Collect gold annotations for `n≈50` dialogues.
2. Weak‑label more data via LIWC + heuristics.
3. Train `FacetPredictor` in PyTorch Lightning (`trainer.fit`).
4. Export weights → `models/facet_predictor.pt`.
5. Re‑train Platt scaler (`calibration.py`) on held‑out set.

---

## 8  Extending

* **Scale to 40 k facets** – increase predictor head, swap `n_facets` env var, and shard LLM rerank (or bypass).
* **Add uncertainty flags** – enable MC‑dropout loops in `predictor.py`.
* **Telemetry** – wrap Gunicorn with Prometheus exporter; log `processing_ms` histogram.
* **Front‑end** – expose `/healthz`, `/metrics`, Swagger docs.

---

## 9  Licence Footnotes

* All included models/libraries are Apache‑2.0, MIT, or GPL‑compatible. No proprietary API keys required.
* Remember: ethical use only; facet scores are probabilistic, not deterministic diagnoses.

---

Happy hacking!  Add or tweak sections to aid Cursor IDE’s context window—its LLM will ingest this README plus inline docstrings to provide smarter completions and refactors.
